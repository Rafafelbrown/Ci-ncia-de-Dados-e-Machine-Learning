{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFIQYuUG1fob",
        "outputId": "079f44b1-17b6-4042-ba8c-a6ef32fc62d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KlCzkBqIspWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dc2a86-9177-40a1-cd57-046b91419b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/wikipedia-pt-br.zip\n",
            "  inflating: /content/ptwiki-latest.json  y\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Comando para descompactar o arquivo ZIP\n",
        "!unzip \"/content/drive/MyDrive/wikipedia-pt-br.zip\" -d \"/content\"\n",
        "\n",
        "# Atualize o caminho do arquivo JSON descompactado\n",
        "PATH = \"/content/ptwiki-latest.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PB502oRsozkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0658420-fe3a-4dd1-e093-a8308561708a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title <class 'str'>\n",
            "section_texts <class 'list'>\n",
            "section_titles <class 'list'>\n",
            "interlinks <class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "PATH = \"/content/ptwiki-latest.json\"\n",
        "import json\n",
        "\n",
        "\"\"\"\n",
        "Rafael Mascarenhas Brown de Andrade\n",
        "Ciência de Dados e Machine Learning\n",
        "Matutino - Asa Norte - Turma: A\n",
        "22304454\n",
        "\"\"\"\n",
        "# Use a codificação detectada para abrir o arquivo\n",
        "with open(PATH) as f:\n",
        "    sample = json.loads(f.readline())\n",
        "\n",
        "for key, value in sample.items():\n",
        "    print(key, type(value))\n",
        "    \"\"\"if key == 'interlinks':\n",
        "        print(value)\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tEHKSnFJCL5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RVjd3FPDJCwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7380cad-a3cc-42e9-a905-2429ce238af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=688a7b108f2f95cfdaaf90e87567ef7fbb401bf565120e56ff027a340649464f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G_6Q0Ymgg6AI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .config(\"spark.executor.memory\", \"25g\") \\\n",
        "    .config(\"spark.driver.memory\", \"25g\") \\\n",
        "    .appName(\"test\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "NO5DNqwRQ7V4",
        "outputId": "4ba32aa2-4fa0-4450-d0a6-68d6c83a1782"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c83026c5ed0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://618f04bc8cd6:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>test</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KGEtNjrMlpWl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "479d80ad-1033-40b7-bc1d-975856b3728f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from pyspark.sql.types import StructType, StructField, StringType, ArrayType\\n\\nschema = StructType([\\n  StructField('title', StringType()),\\n  StructField('section_texts', ArrayType(StringType())), \\n  StructField('section_titles', ArrayType(StringType())), \\n  StructField('interlinks', MapType(StringType(),StringType())) \\n])\\ndf = spark.read.schema(schema).json(PATH)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\"\"\"from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
        "\n",
        "schema = StructType([\n",
        "  StructField('title', StringType()),\n",
        "  StructField('section_texts', ArrayType(StringType())),\n",
        "  StructField('section_titles', ArrayType(StringType())),\n",
        "  StructField('interlinks', MapType(StringType(),StringType()))\n",
        "])\n",
        "df = spark.read.schema(schema).json(PATH)\"\"\""
      ]
    },
    {
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType # Import MapType\n",
        "\n",
        "schema = StructType([\n",
        "  StructField('title', StringType()),\n",
        "  StructField('section_texts', ArrayType(StringType())),\n",
        "  StructField('section_titles', ArrayType(StringType())),\n",
        "  StructField('interlinks', MapType(StringType(),StringType()))\n",
        "])\n",
        "df = spark.read.schema(schema).json(PATH)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "f51i0KGlNJlz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i9AiCwpllyH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1d8227-1f0a-43ab-fc02-91bf857c93cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+--------------------+--------------------+\n",
            "|         title|       section_texts|      section_titles|          interlinks|\n",
            "+--------------+--------------------+--------------------+--------------------+\n",
            "|    Astronomia|[\\nFormação estre...|[Introduction, Hi...|{Calendário julia...|\n",
            "|América Latina|[\\nA '''América L...|[Introduction, Et...|{Altitude -> Elev...|\n",
            "+--------------+--------------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pp1l8XJCJlqo"
      },
      "outputs": [],
      "source": [
        "def read_texts(data):\n",
        "  return json.loads(data.value)[\"section_texts\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hJ4EgJSlmBYG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "rdd= spark.read.text(PATH).rdd.map(read_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KmKpA0o7m5Cd"
      },
      "outputs": [],
      "source": [
        "df_teste= df.select(\"section_texts\").limit(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S9VYB9n2nO-U"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "START = '<|start|>'\n",
        "REPLACEMENTS = [\n",
        "    (r'((?!\\w|{START}).)+', ' '),\n",
        "    (r'\\d+', '<|number|>'),\n",
        "    (r' +', ' '),\n",
        "]\n",
        "\n",
        "def clean(text):\n",
        "    for pattern, repl in REPLACEMENTS:\n",
        "        text = re.sub(pattern, repl, text)\n",
        "    return text.lower()\n",
        "\n",
        "def tokenize(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    return [token for text in texts for token in [START] + clean(text).split()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jBC-HmmGwyTi"
      },
      "outputs": [],
      "source": [
        "# Correct the import statement from 'collecctions' to 'collections'\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def count_words(texte):\n",
        "  words = tokenize(texte)\n",
        "  counts = defaultdict(int)\n",
        "  for word in words:\n",
        "    counts[word] += 1\n",
        "  return list (counts.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K4x5YbDCxAeM"
      },
      "outputs": [],
      "source": [
        "counts2 = rdd.flatMap(count_words)\\\n",
        "  .reduceByKey(lambda acc,x: acc + x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GllTQ5YVDpq4"
      },
      "outputs": [],
      "source": [
        "counts2 = counts2.cache()\n",
        "counts2= counts2.sortBy(lambda x: -x[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zJfSwmYiq40P"
      },
      "outputs": [],
      "source": [
        "a = counts2.take(30000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XIbaUWoCzUcK"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10_000\n",
        "\n",
        "token2id = {token: i for i, (token, _) in enumerate(a[:VOCAB_SIZE])}\n",
        "id2token = {i: token for i, (token, _) in enumerate(a[:VOCAB_SIZE])}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PPkYokFkrC-2"
      },
      "outputs": [],
      "source": [
        " def get_ngrams(text,n=2):\n",
        "  tokens = tokenize(text)\n",
        "  return list(zip(*[tokens[i:] for i in range(n)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cE1DwEOz03vy"
      },
      "outputs": [],
      "source": [
        "def count_bigrams_in_list(texts, filter_vocab=True):\n",
        "  ngrams = get_ngrams(texts,n=2)\n",
        "  counts = defaultdict(lambda: defaultdict(int))\n",
        "  for ngram in ngrams:\n",
        "    if filter_vocab or all(token in token2id for token in ngram):\n",
        "      prev_token, token = ngram\n",
        "      counts [prev_token][token] += 1\n",
        "  return counts.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QYJfaLrd7gx7"
      },
      "outputs": [],
      "source": [
        "def merge_ngrams_count(acc,occ):\n",
        "  for token in occ.keys():\n",
        "    acc[token] = acc[token] + occ[token]\n",
        "  return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_CxZvE4b8J39"
      },
      "outputs": [],
      "source": [
        "rdd = rdd.cache()\n",
        "all_bigrams = rdd.flatMap(count_bigrams_in_list).reduceByKey(merge_ngrams_count).collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u--0WiuW8WWp"
      },
      "outputs": [],
      "source": [
        "\n",
        "bigrams_dict = {key: sorted(value.items(), key=lambda x: x[1], reverse=True) for key, value in all_bigrams}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0375Jz4O8utp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate(text, max_new_tokens=20, top_k=50):\n",
        "\n",
        "    # Tokenize the input text and get the last token\n",
        "    last_token = tokenize(text)[-1]\n",
        "\n",
        "    for i in range(max_new_tokens):\n",
        "        # Get top K probable next tokens and their weights from the bigrams_dict\n",
        "        tokens, weights = zip(*bigrams_dict[last_token][:top_k])\n",
        "\n",
        "        # Normalize the weights to sum up to 1\n",
        "        weights = np.array(weights) / sum(weights)\n",
        "\n",
        "        # Randomly select the next token based on the weights\n",
        "        index = np.argmax(np.random.multinomial(1, weights)).item()\n",
        "\n",
        "        # Check if the selected token is a valid token (not starting with '<|')\n",
        "        if not tokens[index].startswith('<|'):\n",
        "            # Update the last_token and append it to the text\n",
        "            last_token = tokens[index]\n",
        "            text += ' ' + last_token\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gdJE1hX69CBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39a16d20-3941-484e-fd0a-ddb263504e45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'brasil categoria filmes baseados nessas missões secretas de um dos últimos dias depois de sua última prova de uma'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "generate(\"brasil\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}